hadoop的初步使用
	
一、一个简单的单词统计例子

1.1 启动hdfs与yarn 
	start-dfs.sh 
	start-yarn.sh
1.2 准备若干份文件
	text01.txt
	text02.txt
1.3	将准备好的文件放入hdfs文件系统中
	hadoop fs -mkdir -p /wordcount/input
	hadoop fs -put text01.txt text01.txt /wordcount/input
1.4 进入hadoop准备好的demojar文件夹下
	/home/hadoop/apps/hadoop-2.6.4/share/hadoop/mapreduce
	运行
	hadoop jar hadoop-mapreduce-examples-2.6.4.jar wordcount /wordcount/input/ /wordcount/output
	其中注意：/wordcount/input/ 是输入目录，会统计input下的所有文件
	/wordcount/output/ 是输出目录，这个目录在hdfs中不存在，必须不存在，如果存在的话是会报错的


	今天执行到这一步一直有报错主要为以下原因
	1.  yarn没有启动，start-yarn.sh
	2.  在namenode节点配置的机器上，主要ip和主机名字的映射必须存在
		在/etc/hosts中进行配置
		127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
		::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
		192.168.33.129 centmini129
		192.168.33.130 centmini130
		192.168.33.131 centmini131
		192.168.33.132 centmini132
		如果没有这个配置，则start-dfs.sh 是能够正常启动，但是namenode下边的datanode是找不到的
	3.  在最后执行
		hadoop jar hadoop-mapreduce-examples-2.6.4.jar wordcount /wordcount/input/ /wordcount/
		output
		这个命令的时候每次到
		mapreduce.Job: Running job: job_1517129097254_0001
		这一步的时候执行的速度就特别的慢，现在还没有解决